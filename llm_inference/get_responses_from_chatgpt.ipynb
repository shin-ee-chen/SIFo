{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "from xopen import xopen\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pathlib\n",
    "\n",
    "client = OpenAI(api_key=\"#YOUR_API_KEY#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4\"\n",
    "data_file_path = \"../sifo_datasets/math.jsonl\"\n",
    "output_dir = \"../responses/sif_final/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing Your Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(input, dataset):\n",
    "    instructions = []\n",
    "    for i in range(1, 7):\n",
    "        if f\"instruction_{i}\" not in input or \\\n",
    "            len(input[f\"instruction_{i}\"]) < 4:\n",
    "                break\n",
    "        instruction_content= input[f\"instruction_{i}\"]\n",
    "        instructions.append(f\"Instruction_{i}. {instruction_content}\")\n",
    "    instruction_promp = \"\\n\".join(instructions)\n",
    "    \n",
    "    if \"math\" in dataset.lower():\n",
    "        task = 'In the following, you will receive multiple instructions. Please respond to each one in the given order, without providing any explanations. Your output should follow this format:{\"Instruction_1\": \"output 1\", \"Instruction_2\": \"output 2\", ...}'\n",
    "        return f\"{task}\\n{instruction_promp}\"\n",
    "    else:\n",
    "        task = \"In the following, you will receive a context and multiple instructions. Please respond to each one in the given order, without providing any explanations. Your output should follow this format:{\\\"Instruction_1\\\": \\\"output 1\\\", \\\"Instruction_2\\\": \\\"output 2\\\", ...}\"\n",
    "        context = \"Context:\\n\" + input[\"context\"] + \"\\n\" if \"context\" in input else \"\"\n",
    "        return f\"{task}\\n{context}{instruction_promp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_file_dir = \"../batch_files\"\n",
    "task_name = data_file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "batch_file_path = os.path.join(batch_file_dir , f\"{task_name}.jsonl\")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "data = []\n",
    "prompt_data = {}\n",
    "input_data = {}\n",
    "with xopen(data_file_path, 'r') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        input_example = json.loads(line)\n",
    "        id = input_example['id']\n",
    "        input_data[id] = input_example\n",
    "        \n",
    "        user_prompt = create_user_prompt(input_example, task_name)\n",
    "        prompt_data[id] = user_prompt\n",
    "        data_point = {\"custom_id\": str(id), \n",
    "                        \"method\": \"POST\", \n",
    "                        \"url\": \"/v1/chat/completions\", \n",
    "                        \"body\": {\"model\": model, \n",
    "                                \"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                            {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                                \"max_tokens\": 1000}\n",
    "                        }\n",
    "        print(data_point)\n",
    "        data.append(data_point)\n",
    "        \n",
    "\n",
    "\n",
    "with xopen(batch_file_path, \"w\") as f:\n",
    "    for datapoint in data: \n",
    "        f.write(json.dumps(datapoint) + \"\\n\")\n",
    "\n",
    "batch_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Uploading Your Batch Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "  file=open(batch_file_path, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "batch_input_file.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Batch\n",
    "Once you've successfully uploaded your input file, you can use the input File object's ID to create a batch. In this case, let's assume the file ID is file-abc123. For now, the completion window can only be set to 24h. You can also provide custom metadata via an optional metadata parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch_info = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": f\"{data_file_path}\"\n",
    "    }\n",
    ")\n",
    "batch_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Checking the Status of a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(client.batches.list(limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = client.batches.retrieve(batch_info.id)\n",
    "print(status)\n",
    "print(\"output_id\", status.output_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = client.files.retrieve_content(status.output_file_id)\n",
    "responses = responses.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = []\n",
    "output_file_path = os.path.join(output_dir, model, f\"{model}_{task_name}.jsonl\")\n",
    "pathlib.Path(output_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with xopen(output_file_path, \"w\") as f:\n",
    "    for line in responses[:-1]:\n",
    "        response = json.loads(line)\n",
    "        id = int(response['custom_id'])\n",
    "        output_example = deepcopy(input_data[id])\n",
    "        output_example[\"prompt\"] = prompt_data[id]\n",
    "        output_example[\"response\"] = response['response']['body']['choices'][0]['message']['content']\n",
    "        merge_data.append(output_example)\n",
    "        f.write(json.dumps(output_example) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cancelling a Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.cancel(\"#Cancel batch id#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
