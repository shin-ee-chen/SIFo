{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import anthropic\n",
    "from xopen import xopen\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = anthropic.AsyncAnthropic(\n",
    "    api_key=\"#YOUR_API_KEY#\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.datacamp.com/tutorial/getting-started-with-claude-3-and-the-claude-3-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing Your Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(input, dataset):\n",
    "    instructions = []\n",
    "    for i in range(1, 7):\n",
    "        if f\"instruction_{i}\" not in input or \\\n",
    "            len(input[f\"instruction_{i}\"]) < 4:\n",
    "                break\n",
    "        instruction_content= input[f\"instruction_{i}\"]\n",
    "        instructions.append(f\"Instruction_{i}. {instruction_content}\")\n",
    "    instruction_promp = \"\\n\".join(instructions)\n",
    "\n",
    "    if \"math\" in dataset.lower():\n",
    "        task = 'In the following, you will receive multiple instructions. Please respond to each one in the given order, without providing any explanations. Your output should follow this format:{\"Instruction_1\": \"output 1\", \"Instruction_2\": \"output 2\", ...}'\n",
    "        return f\"{task}\\n{instruction_promp}\"\n",
    "    else:\n",
    "        task = \"In the following, you will receive a context and multiple instructions. Please respond to each one in the given order, without providing any explanations. Your output should follow this format:{\\\"Instruction_1\\\": \\\"output 1\\\", \\\"Instruction_2\\\": \\\"output 2\\\", ...}\"\n",
    "        context = \"Context:\\n\" + input[\"context\"] + \"\\n\" if \"context\" in input else \"\"\n",
    "        return f\"{task}\\n{context}{instruction_promp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = \"claude-3-opus-20240229\" \n",
    "data_file_path = \"../sifo_datasets/math.jsonl\"\n",
    "batch_file_dir = \"../batch_files\"\n",
    "task_name = data_file_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "batch_file_path = os.path.join(batch_file_dir, f\"{task_name}.jsonl\")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "requests_params = []\n",
    "input_data = {}\n",
    "prompt_data = {}\n",
    "with xopen(data_file_path, \"r\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        input_example = json.loads(line)\n",
    "        input_data[input_example[\"id\"]] = input_example\n",
    "        user_prompt = create_user_prompt(input_example, task_name)\n",
    "        prompt_data[input_example[\"id\"]] = user_prompt\n",
    "        params = {\n",
    "            \"id\": input_example[\"id\"],\n",
    "            \"model\": model,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        print(params)\n",
    "        requests_params.append(params)\n",
    "\n",
    "with xopen(batch_file_path, \"w\") as f:\n",
    "    for params in requests_params:\n",
    "        f.write(json.dumps(params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list_, n):\n",
    "    for i in range(0, len(list_), n):\n",
    "        yield list_[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_requests_per_minute = 3   # Free Tier rate limit\n",
    "chunked_requests_params = chunks(requests_params[:47], max_requests_per_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_request(request_params):\n",
    "    message = await async_client.messages.create(\n",
    "        model=request_params[\"model\"],\n",
    "        system=request_params[\"system_prompt\"],\n",
    "        messages=request_params[\"messages\"],\n",
    "        max_tokens=request_params[\"max_tokens\"],\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_request_batch():\n",
    "    responses = []\n",
    "    try:\n",
    "        for chunk in chunked_requests_params:\n",
    "            batch_of_responses = await asyncio.gather(\n",
    "                *(send_request(params) for params in chunk)\n",
    "            )\n",
    "            responses.extend(batch_of_responses)\n",
    "            await asyncio.sleep(5)  # sleep for (slightly more than) 1 minute\n",
    "    except anthropic.InternalServerError as e:\n",
    "        if e.status_code == 529:  # API server overloaded\n",
    "            print(\"API server overloaded.\")\n",
    "        else:\n",
    "            raise e\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = asyncio.run(send_request_batch())  # for python script\n",
    "responses = await send_request_batch()  # for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = []\n",
    "output_file_path = Path(\"claude3_generations\", model, f\"{model}_{task_name}_first_47_new.jsonl\")\n",
    "output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with xopen(output_file_path, \"w\") as f:\n",
    "    for request_params, raw_response in zip(requests_params[:47], responses):\n",
    "        id_ = request_params[\"id\"]\n",
    "        output_example = deepcopy(input_data[id_])\n",
    "        output_example[\"prompt\"] = prompt_data[id_]\n",
    "        output_example[\"response\"] = raw_response\n",
    "        merge_data.append(output_example)\n",
    "        f.write(json.dumps(output_example) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
